{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7f25a1",
   "metadata": {},
   "source": [
    "Goal is to explore avgpool for use as a downsampler in our diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c28830",
   "metadata": {},
   "source": [
    "Sections\n",
    "* AvgPool Twice\n",
    "* Variance Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec730720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e6e4c",
   "metadata": {},
   "source": [
    "# AvgPool twice? \n",
    "It seems that we can expect a max error around 2.3842e-07 per dimension. \n",
    "In terms of vectors sizes, we can expect a max error around 1.0476e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afeb804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = lambda x, d: F.avg_pool2d(x,d, d, 0, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4e1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1024\n",
    "N = 64\n",
    "example_N3DimDim = torch.randn(N, 3, dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f566a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twice = D(D(example_N3DimDim, 2), 2)\n",
    "once = D(example_N3DimDim, 4)\n",
    "torch.allclose(twice, once, atol=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29f48c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3842e-07)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(twice-once).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0491e1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0427e-05)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mags = torch.sqrt(torch.sum((twice-once)**2, dim=(1,2,3)))\n",
    "mags.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ae24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgpool_stats_dim(N, dim, scale=1.0): \n",
    "    example_N3DimDim = torch.randn(N, 3, dim, dim) * scale\n",
    "    \n",
    "    twice = D(D(example_N3DimDim, 2), 2)\n",
    "    once = D(example_N3DimDim, 4)\n",
    "    close = torch.allclose(twice, once, atol=1e-7)\n",
    "    print(f\"Close: {close}\", end=\" \")\n",
    "\n",
    "    max_abs_diff = torch.abs(twice-once).max()\n",
    "    print(f\"{max_abs_diff=}\", end=\" \")\n",
    "\n",
    "    mags = torch.sqrt(torch.sum((twice-once)**2, dim=(1,2,3)))\n",
    "    max_magnitude_diff = mags.max()\n",
    "    print(f\"{max_magnitude_diff=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea04ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: True max_abs_diff=tensor(5.9605e-08) max_magnitude_diff=tensor(6.8286e-08)\n",
      "Close: True max_abs_diff=tensor(1.4901e-07) max_magnitude_diff=tensor(2.2781e-07)\n",
      "Close: True max_abs_diff=tensor(1.1921e-07) max_magnitude_diff=tensor(3.7674e-07)\n",
      "Close: True max_abs_diff=tensor(1.7881e-07) max_magnitude_diff=tensor(7.1357e-07)\n",
      "Close: True max_abs_diff=tensor(2.3842e-07) max_magnitude_diff=tensor(1.3735e-06)\n",
      "Close: True max_abs_diff=tensor(2.3842e-07) max_magnitude_diff=tensor(2.6434e-06)\n",
      "Close: True max_abs_diff=tensor(2.3842e-07) max_magnitude_diff=tensor(5.2739e-06)\n",
      "Close: False max_abs_diff=tensor(2.3842e-07) max_magnitude_diff=tensor(1.0474e-05)\n"
     ]
    }
   ],
   "source": [
    "for dim in [4, 16, 32, 64, 128, 256, 512, 1028]: \n",
    "    avgpool_stats_dim(N, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ced375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: False max_abs_diff=tensor(7.6294e-06) max_magnitude_diff=tensor(8.5831e-06)\n",
      "Close: False max_abs_diff=tensor(1.5259e-05) max_magnitude_diff=tensor(2.3253e-05)\n",
      "Close: False max_abs_diff=tensor(1.5259e-05) max_magnitude_diff=tensor(3.7434e-05)\n",
      "Close: False max_abs_diff=tensor(1.5259e-05) max_magnitude_diff=tensor(6.9951e-05)\n",
      "Close: False max_abs_diff=tensor(2.2888e-05) max_magnitude_diff=tensor(0.0001)\n",
      "Close: False max_abs_diff=tensor(3.0518e-05) max_magnitude_diff=tensor(0.0003)\n",
      "Close: False max_abs_diff=tensor(3.0518e-05) max_magnitude_diff=tensor(0.0005)\n",
      "Close: False max_abs_diff=tensor(3.0518e-05) max_magnitude_diff=tensor(0.0010)\n"
     ]
    }
   ],
   "source": [
    "for dim in [4, 16, 32, 64, 128, 256, 512, 1028]: \n",
    "    avgpool_stats_dim(N, dim, scale=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5aa582",
   "metadata": {},
   "source": [
    "I think the larger error can be explained as a small error summed over many entries. It is strange that we get larger errors when scaling by a larger value, but I don't think we should worry about it. The scales are small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9c8a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 0.00114816)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*16, 7.1760e-05 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf14915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgpool_stats_dim_unif(N, dim): \n",
    "    example_N3DimDim = torch.rand(N, 3, dim, dim) * 2 - 1\n",
    "    \n",
    "    twice = D(D(example_N3DimDim, 2), 2)\n",
    "    once = D(example_N3DimDim, 4)\n",
    "    close = torch.allclose(twice, once, atol=1e-7)\n",
    "    print(f\"Close: {close}\", end=\" \")\n",
    "\n",
    "    max_abs_diff = torch.abs(twice-once).max()\n",
    "    print(f\"{max_abs_diff=}\", end=\" \")\n",
    "\n",
    "    mags = torch.sqrt(torch.sum((twice-once)**2, dim=(1,2,3)))\n",
    "    max_magnitude_diff = mags.max()\n",
    "    print(f\"{max_magnitude_diff=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5efd213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: True max_abs_diff=tensor(5.9605e-08) max_magnitude_diff=tensor(6.1889e-08)\n",
      "Close: True max_abs_diff=tensor(8.9407e-08) max_magnitude_diff=tensor(1.3637e-07)\n",
      "Close: True max_abs_diff=tensor(8.9407e-08) max_magnitude_diff=tensor(2.0768e-07)\n",
      "Close: True max_abs_diff=tensor(1.1921e-07) max_magnitude_diff=tensor(3.9192e-07)\n",
      "Close: True max_abs_diff=tensor(1.1921e-07) max_magnitude_diff=tensor(7.3982e-07)\n",
      "Close: True max_abs_diff=tensor(1.4901e-07) max_magnitude_diff=tensor(1.4206e-06)\n",
      "Close: True max_abs_diff=tensor(1.4901e-07) max_magnitude_diff=tensor(2.8310e-06)\n",
      "Close: True max_abs_diff=tensor(1.4901e-07) max_magnitude_diff=tensor(5.6384e-06)\n"
     ]
    }
   ],
   "source": [
    "for dim in [4, 16, 32, 64, 128, 256, 512, 1028]: \n",
    "    avgpool_stats_dim_unif(N, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747e227",
   "metadata": {},
   "source": [
    "These errors disappear when we deal with integer (no decimal point) values: \n",
    "* This indicates that we are running into 32-bit precision problems. I wouldn't worry about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d69bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgpool_stats_dim_arange(N, dim): \n",
    "    example_N3DimDim = torch.arange(N*3*dim*dim).reshape(N, 3, dim, dim)\n",
    "    \n",
    "    twice = D(D(example_N3DimDim, 2), 2)\n",
    "    once = D(example_N3DimDim, 4)\n",
    "    close = torch.allclose(twice, once, atol=1e-7)\n",
    "    print(f\"Close: {close}\", end=\" \")\n",
    "\n",
    "    max_abs_diff = torch.abs(twice-once).max()\n",
    "    print(f\"{max_abs_diff=}\", end=\" \")\n",
    "\n",
    "    mags = torch.sqrt(torch.sum((twice-once)**2, dim=(1,2,3)))\n",
    "    max_magnitude_diff = mags.max()\n",
    "    print(f\"{max_magnitude_diff=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763728b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n",
      "Close: True max_abs_diff=tensor(0) max_magnitude_diff=tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for dim in [4,16,32,64,128, 256, 512,1028]: \n",
    "    avgpool_stats_dim_arange(N, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18810e6",
   "metadata": {},
   "source": [
    "# Variance Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde740d",
   "metadata": {},
   "source": [
    "Now, consider mean-pooling with a kernel and strid of size (a,b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866e148",
   "metadata": {},
   "source": [
    "Suppose we apply it to a Gaussian with isotropic covariance, or \n",
    "$$\n",
    "x \\sim \\mathcal{N}(\\mu, I\\sigma^2).\n",
    "$$\n",
    "After pooling, each entry is comprised of a sample average of $ab$ entries of $x$, or \n",
    "$$\n",
    "    x_{ij}' = \\frac{1}{ab} \\sum_{(k,l) \\in \\text{idx}(i,j)} x_{k,l}.\n",
    "$$\n",
    "This is a sample average over gaussians, so the result is a gaussian as well. \n",
    "\n",
    "The mean is simply the sample mean, but the variance is given by \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(x_{ij}')\n",
    "&= \\text{Var}(\\frac{1}{ab} \\sum_{(k,l) \\in \\text{idx}(i,j)} x_{k,l}) \\\\ \n",
    "&= \\frac{1}{(ab)^2} \\sum_{(k,l) \\in \\text{idx}(i,j)} \\text{Var}(x_{k,l}) \\\\\n",
    "&= \\frac{1}{(ab)^2} (ab) \\text{Var}(x_{k,l}) \\\\ \n",
    "&= \\frac{1}{(ab)^2} (ab) \\sigma^2 \\\\\n",
    "&= \\frac{1}{ab} \\sigma^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "Since the original gaussian is isotropic, and the pooling grabs distinct sets of entries, the covariances are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c5db7",
   "metadata": {},
   "source": [
    "Importantly, if we apply this to noise, the covariance of the noise is scaled by $(1/(ab))$.\n",
    "\n",
    "In order to avoid this, we can\n",
    "* add isotropic noise of variance $(1 - 1/(ab))\\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf9f99",
   "metadata": {},
   "source": [
    "This means that if we want to imagine adding noise at different scales, we need to be careful about how we handle it. We can imagine that downscale operations implicitly add this noise - to keep variance consistent across scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "581c1583",
   "metadata": {},
   "source": [
    "## Example Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1330c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Importing our custom module(s)\n",
    "import unet\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ef9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c21f19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 32, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_NShape = torch.randn((64, 3, 32, 32))\n",
    "t_N = torch.rand((64,))\n",
    "test_net.forwardt_same(t_N, input_NShape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e94b6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net.forwardt(t_N, input_NShape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c371b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give T for each level\n",
    "# we'll define our own betas\n",
    "# assume half size each time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class BasicAvgPoolMultiScaleDiffusion(torch.nn.Module): \n",
    "    \"\"\"\n",
    "        We do diffusion until step T0. Then avgpool. \n",
    "        Then do diffusion until step T1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            Ts: List[int] = []\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.Ts = Ts\n",
    "        \n",
    "        self.Tends = [sum(self.Ts[0:i+1]) for i in range(len(Ts))]\n",
    "        # self.register_buffer(\"betas_A\", torch.linspace(1e-4, 0.02, sum(self.Ts)))\n",
    "        betas_A = torch.linspace(1e-4, 0.02, sum(self.Ts))\n",
    "        self.register_buffer(\"alphas_cumprod_A\", torch.cumprod(1.0 - betas_A, dim=0))\n",
    "\n",
    "\n",
    "    def _xt_given_x0_same(self, x0_NShape: torch.Tensor, level: int, t_N: torch.Tensor): \n",
    "        \"\"\"\n",
    "            All t on the same resolution level.\n",
    "\n",
    "            In this function, we compute x_t, and then get x_{t+1}\n",
    "\n",
    "            t must be below the last t on this level\n",
    "        \"\"\"\n",
    "        # downsize at end of level. \n",
    "        x0_NDown = F.avg_pool2d(x0_NShape, 2**level)\n",
    "        shp = x0_NDown.shape\n",
    "\n",
    "        N = x0_NDown.size(0)\n",
    "\n",
    "        # we are assuming that downsize adds some noise to compensate for \n",
    "        # the variance reduction. SO we can just use betas as is\n",
    "        # computing here, in case we want to make betas learnable\n",
    "        alpha_bar_N = self.alphas_cumprod_A[t_N]\n",
    "\n",
    "        eps_NDown = torch.randn_like(x0_NDown).to(x0_NShape.device)\n",
    "\n",
    "        xt_NDown = (\n",
    "            torch.sqrt(alpha_bar_N)[:, None, None, None] * x0_NDown\n",
    "            + torch.sqrt(1 - alpha_bar_N)[:, None, None, None] * eps_NDown\n",
    "        )\n",
    "        # print(\"same\")\n",
    "        # print(x0_NShape.shape)\n",
    "        # print(xt_NDown.shape)\n",
    "        # print(eps_NDown.shape)\n",
    "        # print(t_N.shape)\n",
    "        return xt_NDown, eps_NDown\n",
    "    \n",
    "    def _xt_given_x0_jump(self, x0_NShape: torch.Tensor, level: int): \n",
    "        \"\"\"\n",
    "            Just came from a jump at level. So at level+1\n",
    "        \"\"\"\n",
    "        # downsize at end of level. \n",
    "        x0_NDown = F.avg_pool2d(x0_NShape, 2**(level))\n",
    "        x0_NSmall = F.avg_pool2d(x0_NDown, 2)\n",
    "\n",
    "        N = x0_NSmall.size(0)\n",
    "\n",
    "        # we are assuming that downsize adds some noise to compensate for \n",
    "        # the variance reduction. SO we can just use betas as is\n",
    "        # computing here, in case we want to make betas learnable\n",
    "\n",
    "        t_val = self.Tends[level]\n",
    "        t_N = torch.full((N,), t_val, dtype=torch.long, device=x0_NShape.device)\n",
    "        alpha_bar_N = self.alphas_cumprod_A[t_N]\n",
    "\n",
    "        eps_NDown = torch.randn_like(x0_NDown).to(x0_NShape.device)\n",
    "        # downscale, so mult by (hw) to keep variance scale\n",
    "        eps_SNSmall = F.avg_pool2d(eps_NDown, 2) * 4\n",
    "\n",
    "        xt_NSmall = (\n",
    "            torch.sqrt(alpha_bar_N)[:, None, None, None] * x0_NSmall\n",
    "            + torch.sqrt(1 - alpha_bar_N)[:, None, None, None] * eps_SNSmall\n",
    "        )\n",
    "        return xt_NSmall, eps_NDown\n",
    "\n",
    "\n",
    "    def xts_given_x0(self, x0_NShape: torch.Tensor, level: int): \n",
    "        N = x0_NShape.size(0) \n",
    "\n",
    "        # this ends up over-emphasizing jumps? \n",
    "        do_jump = torch.rand(1).item() < 0.1\n",
    "\n",
    "        if do_jump: \n",
    "            # from level 0 to 1\n",
    "            xt_NSmall, eps_NSmall = self._xt_given_x0_jump(x0_NShape, 0)\n",
    "            t_end = self.Tends[level]\n",
    "            t_N = torch.full((N,), t_end, dtype=torch.long, device=x0_NShape.device)\n",
    "\n",
    "            return xt_NSmall, eps_NSmall, t_N\n",
    "        \n",
    "        else: \n",
    "            t_end = self.Tends[level]\n",
    "            if level == 0: \n",
    "                t_start = 0 \n",
    "            else: \n",
    "                t_start = self.Tends[level-1] + 1\n",
    "\n",
    "            t_N = torch.randint(t_start, t_end, (N,), device=x0_NShape.device)\n",
    "            xt_NSmall, eps_NSmall = self._xt_given_x0_same(x0_NShape, level, t_N)\n",
    "\n",
    "            return xt_NSmall, eps_NSmall, t_N\n",
    "    \n",
    "\n",
    "    def loss(self, eps_model: UNet, x0_NShape: torch.Tensor): \n",
    "        # N x C x H x W\n",
    "        min_dim = min(x0_NShape.shape[-2:])\n",
    "\n",
    "        import math\n",
    "        shrink = int(math.log(min_dim, 2))\n",
    "\n",
    "        level = int(torch.randint(0, 1 + shrink,  (1,)).item())\n",
    "        xt_NSmall, eps_NSmall, t_N = self.xts_given_x0(x0_NShape, level)\n",
    "\n",
    "        # print(xt_NSmall.shape)\n",
    "        # print(eps_NSmall.shape)\n",
    "        # print(t_N.shape)\n",
    "\n",
    "        if xt_NSmall.shape == eps_NSmall.shape: \n",
    "            eps_pred_NSmall = eps_model.forwardt_same(t_N, xt_NSmall)\n",
    "        else: \n",
    "            eps_pred_NSmall = eps_model.forwardt(t_N, xt_NSmall)\n",
    "        return torch.mean((eps_pred_NSmall - eps_NSmall)**2)\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, eps_model: UNet, xT_NShape: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Reverse DDPM across all timesteps, mapping each t to its corresponding\n",
    "        level using self.Tends. Upscaling happens exactly when t == Tends[level]\n",
    "        (and level > 0), before calling the eps model for that timestep.\n",
    "        \"\"\"\n",
    "        import bisect\n",
    "\n",
    "        device = xT_NShape.device\n",
    "        N = xT_NShape.shape[0]\n",
    "\n",
    "        # Precompute alpha schedules on device\n",
    "        alpha_bar = self.alphas_cumprod_A      # (total_T,)\n",
    "        alpha_bar_prev = torch.cat([torch.tensor([1.0], device=device), alpha_bar[:-1]])\n",
    "\n",
    "        total_T = alpha_bar.shape[0]\n",
    "        x = xT_NShape  # start at final (lowest-res) noise\n",
    "\n",
    "        # We'll iterate global t from total_T-1 down to 0\n",
    "        for t in range(total_T - 1, -1, -1):\n",
    "            # find level for this t: first index i with t <= Tends[i]\n",
    "            # note: self.Tends should be a list of ints\n",
    "            level = bisect.bisect_left(self.Tends, t)\n",
    "            # Safety clamp\n",
    "            if level >= len(self.Tends):\n",
    "                level = len(self.Tends) - 1\n",
    "\n",
    "            # create batch-sized t tensor\n",
    "            t_N = torch.full((N,), t, dtype=torch.long, device=device)\n",
    "\n",
    "            # Choose the appropriate UNet call:\n",
    "            # - If we're at the finest level (level == 0) OR we just upsampled (so x matches fine res),\n",
    "            #   use forwardt_same; otherwise use forwardt.\n",
    "            if (t == self.Tends[level]) and (level < (len(self.Tends) - 1)):\n",
    "                eps_pred = eps_model.forwardt(t_N, x)\n",
    "                x = torch.nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            else:\n",
    "                eps_pred = eps_model.forwardt_same(t_N, x)\n",
    "\n",
    "            # DDPM reverse step (per-batch indexing so broadcasting works)\n",
    "            ab_t = alpha_bar[t_N].view(N,1,1,1)\n",
    "            ab_prev = alpha_bar_prev[t_N].view(N,1,1,1)\n",
    "            a_t = ab_t / ab_prev\n",
    "\n",
    "            coef1 = (1.0 / torch.sqrt(a_t)).view(N, 1, 1, 1)                   # (N,1,1,1)\n",
    "            coef2 = (((1.0 - a_t) / torch.sqrt(1.0 - ab_t))).view(N, 1, 1, 1)  # (N,1,1,1)\n",
    "\n",
    "            mean = coef1 * (x - coef2 * eps_pred)\n",
    "\n",
    "            if t > 0:\n",
    "                sigma_t = torch.sqrt(((1.0 - ab_prev) / (1.0 - ab_t)) * (1.0 - a_t))  # (N,)\n",
    "                sigma_t = sigma_t.view(N, 1, 1, 1)\n",
    "                noise = torch.randn_like(x)\n",
    "                x = mean + sigma_t * noise\n",
    "            else:\n",
    "                x = mean  # at t==0 no noise\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "617ae487",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diff = BasicAvgPoolMultiScaleDiffusion([100, 100, 100, 100, 100, 5])\n",
    "# 32, 16, 8, 4, 2, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e237e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_model = UNet(in_channels=3, num_classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "664821fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfa7ab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.randn((16,3,1,1), device=device)\n",
    "eps_model = eps_model.to(device)\n",
    "test_diff = test_diff.to(device)\n",
    "\n",
    "test_diff.sample(eps_model, start).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4cfe6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_XT_NStart = torch.randn((5, 3, 1,1))\n",
    "# sample_X0_NEnd = test.sample(test_net, test_XT_NStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6edb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_XT_NStart.shape, sample_X0_NEnd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ac7f5",
   "metadata": {},
   "source": [
    "0 = orig\n",
    "99 - last at orig res\n",
    "100 - first downsample and noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd9844",
   "metadata": {},
   "source": [
    "# Generating a Loop with AI for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2e31eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "batch_size = 64\n",
    "lr = 2e-4\n",
    "num_epochs = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset: MNIST 32x32\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),        # rescale to 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # [-1,1]\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ---------------------------\n",
    "# Diffusion parameters\n",
    "# ---------------------------\n",
    "Ts = [100,100,50,50,50,5] \n",
    "diffusion = BasicAvgPoolMultiScaleDiffusion(Ts=Ts).to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# UNet model\n",
    "# ---------------------------\n",
    "eps_model = UNet(in_channels=1, num_classes=1).to(device)\n",
    "optimizer = optim.AdamW(eps_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e66785a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Optional EMA\n",
    "# ---------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {name: p.clone().detach() for name, p in model.named_parameters()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            self.shadow[name].mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            p.data.copy_(self.shadow[name])\n",
    "\n",
    "ema = EMA(eps_model, decay=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f308a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chk = torch.load('checkpoints/checkpoint_epoch_0.pt', map_location=\"cpu\")\n",
    "\n",
    "# eps_model.load_state_dict(chk[\"eps_model\"])\n",
    "# ema = EMA(eps_model)\n",
    "# diffusion.load_state_dict(chk[\"diffusion\"])\n",
    "# optimizer.load_state_dict(chk[\"optimizer\"])\n",
    "\n",
    "# eps_model = eps_model.to(device)\n",
    "# diffusion = diffusion.to(device)\n",
    "\n",
    "# print(\"Loaded checkpoint:\", chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3845d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [02:00<00:00,  7.76it/s, loss=0.0499] \n",
      "Epoch 1: 100%|██████████| 938/938 [02:28<00:00,  6.31it/s, loss=0.118]   \n",
      "Epoch 2: 100%|██████████| 938/938 [01:40<00:00,  9.34it/s, loss=0.107]   \n",
      "Epoch 3: 100%|██████████| 938/938 [01:32<00:00, 10.15it/s, loss=0.0404]  \n",
      "Epoch 4: 100%|██████████| 938/938 [01:42<00:00,  9.17it/s, loss=0.142]   \n",
      "Epoch 5: 100%|██████████| 938/938 [01:32<00:00, 10.10it/s, loss=0.771]   \n",
      "Epoch 6:  80%|███████▉  | 749/938 [01:19<00:19,  9.65it/s, loss=0.0434]  "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}\")\n",
    "    for x0, _ in pbar:\n",
    "        eps_model.train()\n",
    "        \n",
    "        x0 = x0.to(device)  # (N,1,32,32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # diffusion loss\n",
    "        loss = diffusion.loss(eps_model, x0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update(eps_model)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        step += 1\n",
    "\n",
    "        # ---------------------------\n",
    "        # periodic sampling\n",
    "        # ---------------------------\n",
    "        eps_model.eval()\n",
    "        if step % 2000 == 0:\n",
    "            with torch.no_grad():\n",
    "                ema.copy_to(eps_model)\n",
    "                N = 16\n",
    "                noise = torch.randn(N, 1, 1, 1).to(device)\n",
    "                samples = diffusion.sample(eps_model, noise)\n",
    "                samples = samples.clamp(-1,1)\n",
    "                grid = utils.make_grid((samples+1)/2, nrow=4)\n",
    "                os.makedirs(\"samples\", exist_ok=True)\n",
    "                utils.save_image(grid, f\"samples/sample_step_{step}.png\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save checkpoint per epoch\n",
    "    # ---------------------------\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save({\n",
    "        \"eps_model\": eps_model.state_dict(),\n",
    "        \"ema\": ema.shadow,\n",
    "        \"diffusion\": diffusion.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }, f\"checkpoints/checkpoint_epoch_{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026650e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8181fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/782 [00:00<?, ?it/s]ERROR:tornado.general:SEND Error: Host unreachable\n",
      "Epoch 0:   0%|          | 0/782 [01:12<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/782 [01:12<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/782 [01:12<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/782 [01:12<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/782 [01:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     75\u001b[39m     pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (N, 3, 32, 32)\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     75\u001b[39m     pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (N, 3, 32, 32)\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:282\u001b[39m, in \u001b[36mForkProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_fork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:66\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     64\u001b[39m parent_r, child_w = os.pipe()\n\u001b[32m     65\u001b[39m child_r, parent_w = os.pipe()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pid == \u001b[32m0\u001b[39m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:965\u001b[39m, in \u001b[36mcreate_fork.<locals>.new_fork\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    963\u001b[39m     PydevdCustomization.DEFAULT_PROTOCOL = protocol\n\u001b[32m    964\u001b[39m     PydevdCustomization.DEBUG_MODE = debug_mode\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[43m_on_forked_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapply_arg_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_subprocess_fork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     set_global_debugger(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:240\u001b[39m, in \u001b[36m_on_forked_process\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydevd\u001b[39;00m\n\u001b[32m    239\u001b[39m pydevd.threadingCurrentThread().__pydevd_main_thread = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mpydevd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettrace_forked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3341\u001b[39m, in \u001b[36msettrace_forked\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m   3338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clear_thread_local_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3339\u001b[39m     clear_thread_local_info()\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43msettrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_prev_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3013\u001b[39m, in \u001b[36msettrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, overwrite_prev_trace, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, notify_stdin, protocol, **kwargs)\u001b[39m\n\u001b[32m   3010\u001b[39m __setup_holder__ = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m__setup_holder__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3012\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_trace_lock:\n\u001b[32m-> \u001b[39m\u001b[32m3013\u001b[39m     \u001b[43m_locked_settrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_at_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_until_connected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_start_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_end_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3122\u001b[39m, in \u001b[36m_locked_settrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, __setup_holder__, notify_stdin)\u001b[39m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_for_ready_to_run:\n\u001b[32m   3120\u001b[39m     py_db.ready_to_run = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3122\u001b[39m \u001b[43mpy_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3123\u001b[39m py_db.start_auxiliary_daemon_threads()\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:897\u001b[39m, in \u001b[36mPyDB.wait_for_ready_to_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.process_internal_commands()\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m._py_db_command_thread_event.clear()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_py_db_command_thread_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTIMEOUT_FAST\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     75\u001b[39m     pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (N, 3, 32, 32)\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:282\u001b[39m, in \u001b[36mForkProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_fork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:66\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     64\u001b[39m parent_r, child_w = os.pipe()\n\u001b[32m     65\u001b[39m child_r, parent_w = os.pipe()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pid == \u001b[32m0\u001b[39m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:965\u001b[39m, in \u001b[36mcreate_fork.<locals>.new_fork\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    963\u001b[39m     PydevdCustomization.DEFAULT_PROTOCOL = protocol\n\u001b[32m    964\u001b[39m     PydevdCustomization.DEBUG_MODE = debug_mode\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[43m_on_forked_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapply_arg_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_subprocess_fork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     set_global_debugger(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:240\u001b[39m, in \u001b[36m_on_forked_process\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydevd\u001b[39;00m\n\u001b[32m    239\u001b[39m pydevd.threadingCurrentThread().__pydevd_main_thread = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mpydevd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettrace_forked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3341\u001b[39m, in \u001b[36msettrace_forked\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m   3338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clear_thread_local_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3339\u001b[39m     clear_thread_local_info()\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43msettrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_prev_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3013\u001b[39m, in \u001b[36msettrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, overwrite_prev_trace, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, notify_stdin, protocol, **kwargs)\u001b[39m\n\u001b[32m   3010\u001b[39m __setup_holder__ = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m__setup_holder__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3012\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_trace_lock:\n\u001b[32m-> \u001b[39m\u001b[32m3013\u001b[39m     \u001b[43m_locked_settrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_at_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_until_connected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_start_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_end_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3122\u001b[39m, in \u001b[36m_locked_settrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, __setup_holder__, notify_stdin)\u001b[39m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_for_ready_to_run:\n\u001b[32m   3120\u001b[39m     py_db.ready_to_run = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3122\u001b[39m \u001b[43mpy_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3123\u001b[39m py_db.start_auxiliary_daemon_threads()\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:897\u001b[39m, in \u001b[36mPyDB.wait_for_ready_to_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.process_internal_commands()\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m._py_db_command_thread_event.clear()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_py_db_command_thread_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTIMEOUT_FAST\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     75\u001b[39m     pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (N, 3, 32, 32)\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:282\u001b[39m, in \u001b[36mForkProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_fork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:66\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     64\u001b[39m parent_r, child_w = os.pipe()\n\u001b[32m     65\u001b[39m child_r, parent_w = os.pipe()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pid == \u001b[32m0\u001b[39m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:965\u001b[39m, in \u001b[36mcreate_fork.<locals>.new_fork\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    963\u001b[39m     PydevdCustomization.DEFAULT_PROTOCOL = protocol\n\u001b[32m    964\u001b[39m     PydevdCustomization.DEBUG_MODE = debug_mode\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[43m_on_forked_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapply_arg_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_subprocess_fork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     set_global_debugger(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:240\u001b[39m, in \u001b[36m_on_forked_process\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydevd\u001b[39;00m\n\u001b[32m    239\u001b[39m pydevd.threadingCurrentThread().__pydevd_main_thread = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mpydevd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettrace_forked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3341\u001b[39m, in \u001b[36msettrace_forked\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m   3338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clear_thread_local_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3339\u001b[39m     clear_thread_local_info()\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43msettrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_prev_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3013\u001b[39m, in \u001b[36msettrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, overwrite_prev_trace, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, notify_stdin, protocol, **kwargs)\u001b[39m\n\u001b[32m   3010\u001b[39m __setup_holder__ = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m__setup_holder__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3012\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_trace_lock:\n\u001b[32m-> \u001b[39m\u001b[32m3013\u001b[39m     \u001b[43m_locked_settrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_at_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_until_connected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_start_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_end_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3122\u001b[39m, in \u001b[36m_locked_settrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, __setup_holder__, notify_stdin)\u001b[39m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_for_ready_to_run:\n\u001b[32m   3120\u001b[39m     py_db.ready_to_run = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3122\u001b[39m \u001b[43mpy_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3123\u001b[39m py_db.start_auxiliary_daemon_threads()\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:897\u001b[39m, in \u001b[36mPyDB.wait_for_ready_to_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.process_internal_commands()\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m._py_db_command_thread_event.clear()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_py_db_command_thread_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTIMEOUT_FAST\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     75\u001b[39m     pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# (N, 3, 32, 32)\u001b[39;49;00m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/context.py:282\u001b[39m, in \u001b[36mForkProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_fork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/multiprocessing/popen_fork.py:66\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     64\u001b[39m parent_r, child_w = os.pipe()\n\u001b[32m     65\u001b[39m child_r, parent_w = os.pipe()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pid == \u001b[32m0\u001b[39m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:965\u001b[39m, in \u001b[36mcreate_fork.<locals>.new_fork\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    963\u001b[39m     PydevdCustomization.DEFAULT_PROTOCOL = protocol\n\u001b[32m    964\u001b[39m     PydevdCustomization.DEBUG_MODE = debug_mode\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[43m_on_forked_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapply_arg_patch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_subprocess_fork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     set_global_debugger(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:240\u001b[39m, in \u001b[36m_on_forked_process\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydevd\u001b[39;00m\n\u001b[32m    239\u001b[39m pydevd.threadingCurrentThread().__pydevd_main_thread = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[43mpydevd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettrace_forked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m=\u001b[49m\u001b[43msetup_tracing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3341\u001b[39m, in \u001b[36msettrace_forked\u001b[39m\u001b[34m(setup_tracing)\u001b[39m\n\u001b[32m   3338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clear_thread_local_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3339\u001b[39m     clear_thread_local_info()\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43msettrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_prev_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3013\u001b[39m, in \u001b[36msettrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, overwrite_prev_trace, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, notify_stdin, protocol, **kwargs)\u001b[39m\n\u001b[32m   3010\u001b[39m __setup_holder__ = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m__setup_holder__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3012\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_trace_lock:\n\u001b[32m-> \u001b[39m\u001b[32m3013\u001b[39m     \u001b[43m_locked_settrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr_to_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuspend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_only_current_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_at_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_until_connected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_start_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdont_trace_end_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_access_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__setup_holder__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnotify_stdin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:3122\u001b[39m, in \u001b[36m_locked_settrace\u001b[39m\u001b[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, patch_multiprocessing, stop_at_frame, block_until_connected, wait_for_ready_to_run, dont_trace_start_patterns, dont_trace_end_patterns, access_token, client_access_token, __setup_holder__, notify_stdin)\u001b[39m\n\u001b[32m   3119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wait_for_ready_to_run:\n\u001b[32m   3120\u001b[39m     py_db.ready_to_run = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3122\u001b[39m \u001b[43mpy_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_ready_to_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3123\u001b[39m py_db.start_auxiliary_daemon_threads()\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:897\u001b[39m, in \u001b[36mPyDB.wait_for_ready_to_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.process_internal_commands()\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m._py_db_command_thread_event.clear()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_py_db_command_thread_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTIMEOUT_FAST\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "#  Hyperparameters\n",
    "# ---------------------------\n",
    "\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 100\n",
    "\n",
    "# Multi-scale diffusion: e.g. 300 steps at 32x32, then 200 at 16x16, then 200 at 8x8.\n",
    "Ts = [100,100,50,50,50,5]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "#  Dataset: CIFAR10\n",
    "# ---------------------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # [0, 1]\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),    # scale to [-1,1]\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#  Instantiate UNet + diffusion\n",
    "# ---------------------------\n",
    "\n",
    "diffusion = BasicAvgPoolMultiScaleDiffusion(Ts=Ts).to(device)\n",
    "\n",
    "# Instantiate your UNet (you must adapt this to your constructor)\n",
    "eps_model = UNet().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(eps_model.parameters(), lr=lr)\n",
    "\n",
    "# Optional: EMA\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {\n",
    "            name: p.clone().detach()\n",
    "            for name, p in model.named_parameters()\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            self.shadow[name].mul_(self.decay).add_(p.data, alpha=1 - self.decay)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            p.data.copy_(self.shadow[name])\n",
    "\n",
    "ema = EMA(eps_model, decay=0.999)\n",
    "\n",
    "# ---------------------------\n",
    "#  Training Loop\n",
    "# ---------------------------\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for x0, _ in pbar:\n",
    "\n",
    "        x0 = x0.to(device)                # (N, 3, 32, 32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # The diffusion object handles picking a random level,\n",
    "        # generating xt, eps, t, and computing loss.\n",
    "        loss = diffusion.loss(eps_model, x0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update(eps_model)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        step += 1\n",
    "\n",
    "        # -------------------------------------------\n",
    "        #  Periodic sampling\n",
    "        # -------------------------------------------\n",
    "        if step % 2000 == 0:\n",
    "            with torch.no_grad():\n",
    "                # use EMA weights for sampling quality\n",
    "                ema.copy_to(eps_model)\n",
    "\n",
    "                N = 16\n",
    "                # start from noise matching the lowest resolution\n",
    "                # lowest resolution = 32 // (2 ** (len(Ts)-1))\n",
    "                lowest_res = 32 // (2 ** (len(Ts)-1))   # e.g. 8\n",
    "                noise = torch.randn(N, 3, lowest_res, lowest_res).to(device)\n",
    "\n",
    "                samples = diffusion.sample(eps_model, noise)\n",
    "                samples = samples.clamp(-1, 1)\n",
    "\n",
    "                # Save samples\n",
    "                grid = torchvision.utils.make_grid((samples+1)/2, nrow=4)\n",
    "                torchvision.utils.save_image(grid, f\"samples_step_{step}.png\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    #  Save checkpoint at each epoch\n",
    "    # -------------------------------------------\n",
    "    torch.save({\n",
    "        \"eps_model\": eps_model.state_dict(),\n",
    "        \"ema\": ema.shadow,\n",
    "        \"diffusion\": diffusion.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }, f\"checkpoint_epoch_{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631bf3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01895edf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b005c7c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44fc4d90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b43f872c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
